\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Weekly Report(February.25,2019-March.1,2019)}
\author{Rui Shaopu}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}
\maketitle

\begin{abstract}
During the winter holiday, I learned several optimization algorithms first, then I also learnt a lot about classifiers and finished assignment 1 of cs231n.
\end{abstract}

\section{Learnt and Did}

Mainly I learned about classifiers and optimization algorithms. And I finished assignment 1, which gives me a big challenge, since I'm not familiar with python.

\subsection{Optimization Algorithms}
\subsubsection{Gradient Descent}
Usually we use this way to find the maximum value or the minimum value. We shall make it when the function is convex function.

Main formula:
\begin{center}
${\displaystyle J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(y_i-h_\theta(x_i))^2}$ 
\end{center}
partial derivation:
\begin{center}
${\displaystyle \theta'_j=\theta_j+(y_j-h_\theta(x_i))x_j^i}$
\end{center}
To get the value we want, we need to update parameters in the oppsite direction of gradient.

\subsubsection{Logistic Regression}
It is usually used to dedal with two classification problems since its unique attributes.

Main formula:
\begin{center}
${\displaystyle Y_w=\frac{1}{1+e^{-W^TX}}}$
\end{center}
\begin{center}
${\displaystyle L(W)=-\frac{1}{m}\log({\prod_{i=1}^{m}[Y_w(X_i)]^{y_i}[1-Y_w(X_i)]^{1-y_i}})}$
\end{center}
Iterative formula($\alpha$ is learning rate):
\begin{center}
${\displaystyle W_j=\frac{\alpha}{m}\sum_{i=1}^{m}(Y_w(xi)-y_i)x_{ij}}$
\end{center}

\subsubsection{Newton's method}
Newton's method drops faster than gradient descent. However, it works bad when dealing with hessian matrix. 

Main formula:
\begin{center}
${\displaystyle x_{n+1}=x_n-\frac{f'(x_n)}{f"(x_n)}}$
\end{center}

Hessian matrix:
\begin{center}
${\displaystyle x_{n+1}=x_n-\frac{J_f(x_n)}{H(x_n)}}$

${\displaystyle J_f(x_n)=
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1}      & \cdots & \frac{\partial y_1}{\partial x_n}      \\
\vdots & \ddots & \vdots \\
\frac{\partial y_n}{\partial x_1}      & \cdots & \frac{\partial y_n}{\partial x_n}
\end{bmatrix}
}$

${\displaystyle H(x_n)=
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2}   & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n}      \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1}   & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
}$
\end{center}

\subsection{Classifiers}

\subsubsection{KNN}
KNN, K Nearest Neighbor, One of the most easiest classifiers.

\begin{figure}[h]
	\centering  %插入的图片居中表示
	\includegraphics[width=.4\textwidth]{5.png} 
	\caption{2-dimension}  %图片的名称
	\label{fig:f5}   %标签，用作引用	
\end{figure}
In the training process of this algorithm, it stores all the input and output labels of training samples. in the test process, the distance between the test sample and each training sample L1 or L2 is calculated, then the first k training sample closest to test sample are selected. Then we'll vote on the label of the k samples, the category with the largest number of votes is categorized as test samples.

KNN is very easy, however it gets a bad result with classification accuracy of 27 percent. 

\subsubsection{SVM}
SVM, Support Vector Machine, is a kind of generalized linear classifier which classifies data by supervised learning. Its decision boundary is the maximum margin hyperplane for solving learning samples.

Under normal conditions, we use linear-SVM to classify two kinds of data, between which there is an obvious interval. However, if it doesn't exist, we need to find it in higher dimension. The figures below show it.

\begin{figure}[h]
	\centering  %插入的图片居中表示
	\includegraphics[width=.4\textwidth]{1.jpg} 
	\caption{2-dimension}  %图片的名称
	\label{fig:f1}   %标签，用作引用	
\end{figure}
\begin{figure}[h]
	\centering  %插入的图片居中表示
	\includegraphics[width=.4\textwidth]{2.jpg}
	\caption{higher dimension}  %图片的名称
	\label{fig:f2}   %标签，用作引用
\end{figure}	

\subsubsection{Softmax}
In mathematics, the softmax function, also known as softargmax or normalized exponential function, is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities. 

That is, after applying softmax, each component will be in the interval(0,1), and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. Softmax is often used in neural networks and Often acts as an activation function of a neural network


The standard (unit) softmax function ${\displaystyle \sigma : {R} ^{K}\to {R} ^{K}}$ is defined by the formula

${\displaystyle \sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}}$
for j = 1, …, K and ${\displaystyle {\mathbf {z}}=(z_{1},\ldots ,z_{K})\in  {R} ^{K}}$


\subsubsection{Neural Network}
Neural network is a very important part of machine learning. As I know, Convolutional neural network is the most powerful one. In assignment 1, I need to finish a two-layer neural network. 

Formula: 

Hyperplane: ${\displaystyle y = W X+b}$

Activation function: ${\displaystyle f_{sigmod} = \frac{1}{1+e^{z}}}$

Activation function derivation: ${\displaystyle f'_{sigmod} = f_{sigmod}(1-f_{sigmod})}$

\begin{figure}[h]
	\centering  %插入的图片居中表示
	\includegraphics[width=.4\textwidth]{4.png} 
	\caption{2-dimension}  %图片的名称
	\label{fig:f3}
\end{figure}

\section{Problems}
The best problem I got is that I'm not familiar with Python libraries, such as numpy, matplotlib and so on. So was matrix operation. It wasted me a lot of time dealing with them and they still troubles me too.

\section{Plan for Next Weeks}
In the following weeks, I will keep going and finish assignment 2. I will still learn to use python and latex familiarly.

\end{document}