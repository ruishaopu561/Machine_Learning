## Deep Learning
### K最近邻算法
(K-Nearest Neighbour, KNN)  
在模式识别中，KNN算法是一种用于分类和回归的非参数统计方法。 
  
KNN算法的核心思想：如果一个样本在特征空间中的K个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。KNN方法在类别决策上仅仅依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。  
  
距离度量表示法：
```
(1)、欧式距离；
(2)、曼哈顿距离；
(3)、切比雪夫距离；
(4)、闵可夫斯基距离；
(5)、标准化欧式距离；
(6)、马氏距离；
(7)、巴氏距离；
(8)、汉明距离；等。
```
### 损失函数，正则化
#### 损失函数
#### 正则化
正则化参数要做的就是控制两个目标之间的平衡关系：在最小化训练误差的同时正则化参数使模型简单。  
```
1.最小化误差是为了更好的拟合训练数据。
2.正则化参数是为了防止模型过分拟合训练数据。
```
所以正则化参数要保证模型简单的基础上使模型具有很好的泛化性能。
正则化方法
L_0范数：向量中非零元素的个数，若用L_0范数来规则化参数矩阵，就是希望参数矩阵大部分元素都为0，使特征矩阵稀疏。但是很难优化求解。  
L_1范数：向量中各个绝对值元素之和。因为参数的大小与模型的复杂度成正比，所以模型越复杂，范数就越大。使用范数也可以实现特征稀疏。正则化是正则化的最优凸近似，相对范数的优点是容易求解，因此通常都是用范数进行正则化，而不是范数。　　
L_2范数：各参数的平方和在求平方根。让L_2范数的正则化项‖W‖_2最小，可以使W的每个元素都很小，都接近于零。但是与范数L_1不同，它不使每个元素都为0，而是接近于零。越小的参数模型越简单，越简单的模型越不容易产生过拟合现象。
### 神经网络
#### 一般的神经网络
BP算法，通过计算当前输出与理论输出的差向后传播，根据每个参数的导数计算出差值x，然后`w += a*x`更新参数，以此递归很多次，直至差值最小。  
[实现](https://github.com/ruishaopu561/Machine_Learning/blob/master/NeuralNetworks.md)
#### 卷积神经网络
卷积神经网络是神经网络输入太大时的一种新的处理方式，原理是，经过卷积和池化的数据仍能反映原数据的特征。
每一层卷积层都有超过一个的卷积核，卷积核很小，每一个核都代表一种基本特征，用于平移得到新的元素，池化。
#### 激活函数
输入和参数点积之后并加上偏置项，然后带入激活函数计算的结果即可作为下一层的输入。
##### sigmoid 
缺点：  
1、导数值范围为(0,0.25]，反向传播时会导致“梯度消失“。tanh函数导数值范围更大，相对好一点。  
2、sigmoid函数不是0中心对称，tanh函数是，可以使网络收敛的更好。  
3、没有中心化，而且容易饱和，当ｘ较大时就会出现饱和，即`y=1`或`y=0`。    
求导：`y'=y(1-y)`
##### tanh
相比sigmoid函数有所改善，有中心化，同时还是容易饱和。
求导：`y'=1-y*y `
##### ReLu
`y=max(0, x)`
解决了饱和的问题，但是舍去了所有的负值，即一半的输入。
##### Leaky_ReLu
`y=max(ax, x)`
#### 批量归一化
在进行训练之前，先将数据进行预处理，方法是计算某样本均值、方差，然后将所有数据，都计算得到正态化的分布值。即为归一化。
#### 机器学习常用工具
Caffe, Torch, Theano, Tensorflow, Keras, PyTorch...
#### 几种卷积神经网络
##### AlexNet
##### VGG
##### GoogLeNet
##### ResNet
#### 几种神经网络(RNN, LSTM, GRU)
##### RNN
RNN(Recurrent Neural Network)  
BPTT(back-propagation through time)
##### LSTM
(long short-term memory)
#### Pixel, RNN, CNN
[RNN](https://blog.csdn.net/zhaojc1995/article/details/80572098)
#### 变自分编码器
#### 生成式对抗网络
#### Q-Learning, Actor-Critic算法
#### 对抗样本和对抗训练
